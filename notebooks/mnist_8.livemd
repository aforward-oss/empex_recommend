# Is your number the number 8?

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:scidata, "~> 0.1"}
])
```

## Our Tensor helper (Tx)

A few things that I didn't see a _nice_ way to handle, I put into a `Tx` helper (for Tensor Nx helper).

```elixir
defmodule MnistMl do
  def download() do
    {Scidata.MNIST.download(), Scidata.MNIST.download_test()}
  end

  def to_features(data) do
    {bin, type, shape} = data

    {dataset_size, _channels, h, w} = shape
    num_features = h * w

    pixels =
      bin
      |> Nx.from_binary(type)
      |> Nx.divide(255.0)
      |> Nx.reshape({dataset_size, num_features})

    bias = Nx.broadcast(1, {dataset_size, 1})
    Nx.concatenate([bias, pixels], axis: 1)
  end

  def to_labels(data) do
    {bin, type, _} = data

    bin
    |> Nx.from_binary(type)
    |> Nx.new_axis(-1)
    |> Nx.equal(Nx.tensor([8]))
  end

  def train(images, labels, opts \\ []) do
    iterations = opts[:iterations] || 10
    lr = opts[:lr] || 0.01
    initial_weights = Nx.broadcast(0, Nx.shape(images))

    Enum.reduce(1..iterations, initial_weights, fn _, w ->
      delta =
        gradient(images, labels, w)
        |> Nx.multiply(Nx.tensor(lr))

      Nx.subtract(w, delta)
    end)
  end

  def forward(x, w) do
    x
    |> Nx.dot(w)
    |> sigmoid()
  end

  def sigmoid(x) do
    one = Nx.tensor(1.0)
    Nx.divide(one, Nx.add(one, Nx.exp(Nx.negate(x))))
  end

  def gradient(x, y, w) do
    err = forward(x, w) |> Nx.subtract(y)

    Nx.transpose(x)
    |> Nx.dot(err)
    |> Nx.divide(Nx.shape(x))
  end
end
```

## Download Images

Scidata provides a convenient mechanism to download popular data sets.  Let's grab [MNSIT](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) database using [Scidata.MNIST.download()](https://github.com/elixir-nx/scidata/blob/master/lib/scidata/mnist.ex) and `download_test`. We wrapped that above in our `MnistMl.download/0` helper.

```elixir
{train, test} = MnistMl.download()
```

### Prepare Training Images

What's nice is that the dataset is already split for us so we fully analyze the training set.  But we need to _clean up_ the data for our _naive_ analysis using linear regression.  The binary data needs to be read, normalized down to 0s and 1s, flattened from a 28x28 to a 1x784 and finally we want to add a _bias_ feature.

```elixir
{raw_images, raw_labels} = train
train_images = MnistMl.to_features(raw_images)
```

### Prepare Labels

The labels are 0 to 9.  To make binary decisions on correctness we want to one-hot encode them using the `Nx.equal` helper.

```elixir
train_labels = MnistMl.to_labels(raw_labels)
```

## Let's build a classifier for our data

```elixir
# w = MnistMl.train(train_images, train_labels, iterations: 3, lr: 0.1)
MnistMl.sigmoid(Nx.tensor([1, 2, 3]))
```
